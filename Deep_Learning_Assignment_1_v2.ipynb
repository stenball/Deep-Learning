{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning - Assignment 1_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yNfAbGRBjg0",
        "colab_type": "text"
      },
      "source": [
        "##Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdSEb0NMBqv6",
        "colab_type": "text"
      },
      "source": [
        "If you are marking this assignment and want to use the test data make sure marker_mode is set to True and provide the test data and test labels below.\n",
        "Otherwise set marker mode to False."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmCure2kB8Ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "marker_mode = True\n",
        "# Provide test data and true test labels here\n",
        "test_data = None\n",
        "test_labels = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKlXRb8TUuy-",
        "colab_type": "text"
      },
      "source": [
        "##Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIXvAbr3wVro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import bernoulli\n",
        "import h5py\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as pl\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g-Tb_Y_VFn5",
        "colab_type": "text"
      },
      "source": [
        "##Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OYY6lFiSucI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Google drive id of the data\n",
        "train_data_id = '1CAqSwLA1yibqBr5psO6v-FpW9DQFB6ud'\n",
        "train_labels_id = '1qMwn7TM8reBqdrWKiPCi8jtRLNPRKc7w'\n",
        "\n",
        "# Download training data\n",
        "downloaded = drive.CreateFile({'id':train_data_id}) \n",
        "downloaded.GetContentFile('train_128.h5')\n",
        "with h5py.File('train_128.h5','r') as H:\n",
        "    data = np.copy(H['data'])\n",
        "\n",
        "# Download training labels\n",
        "downloaded = drive.CreateFile({'id':train_labels_id}) \n",
        "downloaded.GetContentFile('train_label.h5')\n",
        "with h5py.File('train_label.h5','r') as H:\n",
        "    label = np.copy(H['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AQ-24nVVU12",
        "colab_type": "text"
      },
      "source": [
        "##Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyxY_3HyVu3H",
        "colab_type": "text"
      },
      "source": [
        "###Normalize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yED9rYylL5lM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data(data):\n",
        "    nom = (data-data.min(axis=0))*(2)\n",
        "    denom = data.max(axis=0) - data.min(axis=0)\n",
        "    denom[denom==0] = 1\n",
        "    data = -1 + nom/denom\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohG1Qk7lV01-",
        "colab_type": "text"
      },
      "source": [
        "### Split into training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX7hiDiQxwni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if marker_mode:\n",
        "    data = normalize_data(data)\n",
        "    X_train = data\n",
        "    y_train = label\n",
        "    if test_data is not None:\n",
        "        test_data = normalize_data(test_data)\n",
        "        X_test = test_data\n",
        "        y_test = test_labels\n",
        "    else:\n",
        "        X_test = None\n",
        "        y_test = None\n",
        "else:\n",
        "    data = normalize_data(data)\n",
        "    #Stratified splitting of the data into train and test sets.\n",
        "    y=np.array(label)\n",
        "    train_index = np.zeros(len(y),dtype=bool)\n",
        "    test_index = np.zeros(len(y),dtype=bool)\n",
        "    values = np.unique(y)\n",
        "    for value in values:\n",
        "        value_index = np.nonzero(y==value)[0]\n",
        "        np.random.shuffle(value_index)\n",
        "        n = int(0.7*len(value_index))\n",
        "        train_index[value_index[:n]]=True\n",
        "        test_index[value_index[n:]]=True\n",
        "\n",
        "    X_train = data[train_index]\n",
        "    y_train = label[train_index]\n",
        "    X_test =  data[test_index]\n",
        "    y_test = label[test_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGOpGy8LWWW-",
        "colab_type": "text"
      },
      "source": [
        "### Transform labels to one-hot-encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhGMJ0y3L6Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#One-Hot-Encoding the labels of both the train and the test sets\n",
        "train_label_OHE = np.zeros((y_train.size, y_train.max()+1))\n",
        "train_label_OHE[np.arange(y_train.size),y_train] = 1\n",
        "y_train = train_label_OHE\n",
        "\n",
        "if y_test is not None:\n",
        "    test_label_OHE = np.zeros((y_test.size, y_test.max()+1))\n",
        "    test_label_OHE[np.arange(y_test.size),y_test] = 1\n",
        "    y_test = test_label_OHE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHy3YAehWtAP",
        "colab_type": "text"
      },
      "source": [
        "##Create the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAr8yAdjWwk1",
        "colab_type": "text"
      },
      "source": [
        "###Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w6vIwnHZLJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Activation(object):\n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def __tanh_deriv(self, a):\n",
        "        return 1.0 - a**2\n",
        "\n",
        "    def __logistic(self, x):\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    def __logistic_deriv(self, a):\n",
        "        return  a * (1 - a )\n",
        "\n",
        "    def __relu(self, x):\n",
        "        return x * (x > 0)\n",
        "\n",
        "    def __relu_deriv(self, a):\n",
        "        return (a > 0)\n",
        "\n",
        "    def __softmax_old(self, x):\n",
        "        exp = np.exp(x - np.max(x))\n",
        "        return exp / (np.sum(exp) + 1e-6)\n",
        "\n",
        "    def __softmax(self, x):\n",
        "        if len(x.shape) > 1:\n",
        "            out = np.zeros(x.shape)\n",
        "            for i in range(x.shape[0]):\n",
        "                exp = np.exp(x[i,:] - np.max(x[i,:]))\n",
        "                out[i,:] = exp / np.sum(exp)\n",
        "        else:\n",
        "            exp = np.exp(x - np.max(x))\n",
        "            out = exp / (np.sum(exp))\n",
        "        return out\n",
        "\n",
        "    def __softmax_deriv_old(self, a):\n",
        "        sm = self.__softmax(a)\n",
        "        return sm*(1 - sm)\n",
        "\n",
        "    def __softmax_deriv(self, a):\n",
        "        return a*(1 - a)\n",
        "\n",
        "    def __init__(self,activation='relu'):\n",
        "        if activation == 'logistic':\n",
        "            self.f = self.__logistic\n",
        "            self.f_deriv = self.__logistic_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv\n",
        "        elif activation == 'softmax':\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA0kwnJ8XN48",
        "colab_type": "text"
      },
      "source": [
        "###Standard hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-NuH11LahFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HiddenLayer(object):\n",
        "    def __init__(self, n_in, n_out, activation_last_layer='relu', activation='relu', dropout=0, W=None, b=None):\n",
        "        self.input=None\n",
        "        self.output=None\n",
        "        self.activation=Activation(activation).f\n",
        "        self.activation_deriv=None\n",
        "        if activation_last_layer:\n",
        "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
        "\n",
        "        # Modified Kaiming normal initialization\n",
        "        self.W = np.random.normal(scale=np.sqrt(2/(n_in + n_out)), size=(n_in,n_out))\n",
        "        self.b = np.zeros(n_out,)\n",
        "        \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        self.weight_momentum = np.zeros(self.W.shape)\n",
        "        self.bias_momentum = np.zeros(self.b.shape)\n",
        "        self.dropout = dropout\n",
        "        self.dropout_distribution = None\n",
        "\n",
        "    def forward(self, input, train_mode=True, output_layer=False):\n",
        "        # Apply linear transformation to input\n",
        "        # If not training, multiply weights by dropout factor\n",
        "        if (not train_mode) and self.dropout:\n",
        "            lin_output = np.dot(input, self.W*(1-self.dropout)) + self.b*(1-self.dropout)\n",
        "        else:\n",
        "            lin_output = np.dot(input, self.W) + self.b\n",
        "\n",
        "        # Apply activation function\n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output))\n",
        "\n",
        "        # Apply dropout if this is not output layer and network is in training mode\n",
        "        if train_mode and self.dropout and not output_layer:\n",
        "            self.dropout_distribution = dropout_distribution = bernoulli.rvs(1-self.dropout, size=self.b.size)\n",
        "            self.output = self.dropout_distribution * self.output   \n",
        "        \n",
        "        self.input=input\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, output_layer=False):  \n",
        "        # Calculate gradients\n",
        "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
        "        self.grad_b = np.sum(delta,axis=0)\n",
        "\n",
        "        # Calculate the sensitivity to pass backwards\n",
        "        if self.activation_deriv:\n",
        "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        return delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOYFpJ3gXSfg",
        "colab_type": "text"
      },
      "source": [
        "###Batch normalization layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zVQ4dYfw5L6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BNLayer(object):\n",
        "    def __init__(self, n_nodes):\n",
        "        self.input=None\n",
        "        self.output=None\n",
        "\n",
        "        # Initialize W to ones and b to zeros\n",
        "        self.W = np.ones(n_nodes,)\n",
        "        self.b = np.zeros(n_nodes,)\n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        self.bias_momentum = 0\n",
        "        self.weight_momentum = 0\n",
        "        self.mean = None\n",
        "        self.var = None\n",
        "\n",
        "    def forward(self, input, train_mode=True):\n",
        "        # Update mean, variance and layer input if network is in training mode\n",
        "        if train_mode:\n",
        "            self.mean = np.mean(input, axis=0)\n",
        "            self.var = np.var(input, axis=0)\n",
        "            self.input = input\n",
        "\n",
        "        # Normalize input\n",
        "        norm_input = (input - self.mean) / np.sqrt(self.var + 1e-3)\n",
        "\n",
        "        # Apply linear transformation\n",
        "        output = norm_input * self.W + self.b\n",
        "        return output\n",
        "\n",
        "    def backward(self, delta, output_layer=False):\n",
        "        batch_size = self.input.shape[0]\n",
        "        X_centered = self.input - self.mean\n",
        "        X_norm = X_centered / np.sqrt(self.var + 1e-3)\n",
        "        std_inv = 1 / np.sqrt(self.var + 1e-3)\n",
        "        \n",
        "        # Calculate gradients\n",
        "        self.grad_b = np.sum(delta, axis=0)\n",
        "        self.grad_W = np.sum(delta*X_norm, axis=0)\n",
        "\n",
        "        # Calculate sensitivity. Based on\n",
        "        # https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
        "        delta = 1 / batch_size * self.W * std_inv * (batch_size * delta\n",
        "            - np.sum(delta, axis=0) - X_centered * std_inv**2 * np.sum(delta * X_centered, axis=0))\n",
        "        return delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGrcRrV0XVqw",
        "colab_type": "text"
      },
      "source": [
        "###The MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ujaUwqXfqqFv",
        "colab": {}
      },
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers, activation, momentum_term=0.9, weight_decay=0, dropout=0, batch_normalization=False):\n",
        "        self.layers=[]\n",
        "        self.params=[]\n",
        "        self.activation=activation\n",
        "\n",
        "        # Add layers and activation functions to network\n",
        "        for i in range(len(layers)-1):\n",
        "            # Add batch normalization layers between other layers if wanted\n",
        "            if batch_normalization:\n",
        "                self.layers.append(BNLayer(layers[i]))\n",
        "            \n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1], dropout=dropout))\n",
        "        self.batch_normalization = batch_normalization\n",
        "        self.momentum_term = momentum_term\n",
        "        self.decay_factor = 1 - weight_decay\n",
        "\n",
        "    def forward(self,input, train_mode=True):\n",
        "        # Forward propagation of input through network\n",
        "        for layer in self.layers[:-1]:\n",
        "            output=layer.forward(input, train_mode)\n",
        "            input=output\n",
        "        \n",
        "        # Output layer is a special case\n",
        "        output=self.layers[-1].forward(input, train_mode, output_layer=True)\n",
        "        return output\n",
        "\n",
        "    def criterion_MSE(self,y,y_hat):\n",
        "        activation_deriv = Activation(self.activation[-1]).f_deriv\n",
        "        error = y-y_hat\n",
        "        loss = np.dot(error, error)\n",
        "        delta = -error*activation_deriv(y_hat)    \n",
        "        return loss,delta\n",
        "\n",
        "    def cross_entropy(self,y,y_hat):\n",
        "        loss = -np.dot(y, np.log(y_hat + 1e-9))\n",
        "        delta = y_hat-y\n",
        "        activation_deriv = Activation(self.activation[-1]).f_deriv\n",
        "        delta = delta*activation_deriv(y_hat)\n",
        "        return loss,delta\n",
        "\n",
        "    def backward(self,delta):\n",
        "        # Output layer is a special case\n",
        "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
        "\n",
        "        # Don't backward propagate to input layer\n",
        "        if self.batch_normalization:\n",
        "            layers = self.layers[1:-1]\n",
        "        else:\n",
        "            layers = self.layers[:-1]\n",
        "\n",
        "        # Backward propagation through network\n",
        "        for layer in reversed(layers):\n",
        "            delta=layer.backward(delta)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        # Update parameters for all layers with momentum and weight decay if specified\n",
        "        for layer in self.layers:\n",
        "            layer.weight_momentum = self.momentum_term * layer.weight_momentum + lr * layer.grad_W\n",
        "            layer.W -= layer.weight_momentum\n",
        "            layer.W = layer.W * self.decay_factor\n",
        "\n",
        "            layer.bias_momentum = self.momentum_term * layer.bias_momentum + lr * layer.grad_b\n",
        "            layer.b -= layer.bias_momentum\n",
        "            layer.b = layer.b * self.decay_factor\n",
        "\n",
        "    def fit(self,X,y,learning_rate=0.001, batch_size=32, epochs=50, eval_mode=False, X_test=None, y_test=None):\n",
        "        X=np.array(X)\n",
        "        y=np.array(y)\n",
        "\n",
        "        # Store training and testing accuracies if network is in evaluation mode\n",
        "        if eval_mode:\n",
        "            to_return = np.zeros((epochs,3))\n",
        "        else:\n",
        "            to_return = np.zeros(epochs)\n",
        "        \n",
        "        for k in range(epochs):\n",
        "\n",
        "            # Number of iterations in each epoch depends on batch size\n",
        "            n_iterations = int(np.floor(X.shape[0] / batch_size))\n",
        "\n",
        "            # Store loss, then average for each epoch\n",
        "            loss=np.zeros((n_iterations,batch_size))\n",
        "\n",
        "            # Randomly select a sequence of indices\n",
        "            epoch_indices = np.random.choice(range(X.shape[0]), size=X.shape[0], replace=False)\n",
        "\n",
        "            for it in range(n_iterations):\n",
        "                # Select batch indices from the epoch-wide sequence of indices\n",
        "                batch_indices = epoch_indices[it*batch_size:(it+1)*batch_size]\n",
        "\n",
        "                # Forward propagate through network\n",
        "                y_hat = self.forward(X[batch_indices])\n",
        "\n",
        "                # Store deltas and calculate average batch loss\n",
        "                delta = []\n",
        "                for i in range(batch_size):\n",
        "                    #y_hat = self.forward(X[i])\n",
        "                    if self.activation[-1] == 'softmax':\n",
        "                        temp_loss,temp_delta = self.cross_entropy(y[batch_indices[i],:], y_hat[i,:])\n",
        "                    else:\n",
        "                        temp_loss,temp_delta = self.criterion_MSE(y[batch_indices[i],:], y_hat[i,:])\n",
        "                    loss[it] += temp_loss / batch_size\n",
        "                    delta.append(temp_delta)\n",
        "\n",
        "                # Backward propagate through network\n",
        "                self.backward(np.array(delta))\n",
        "\n",
        "                # Update parameters\n",
        "                self.update(learning_rate)\n",
        "\n",
        "            # Store wanted values\n",
        "            if eval_mode:\n",
        "                train_accuracy = accuracy_score(self.predict(X_train), y_train)\n",
        "                test_accuracy = accuracy_score(self.predict(X_test), y_test)\n",
        "                epoch_results = np.array([np.mean(loss), train_accuracy, test_accuracy])\n",
        "                to_return[k,:] = epoch_results\n",
        "            else:\n",
        "                to_return[k] = np.mean(loss)\n",
        "        return to_return\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = np.array(x)\n",
        "        output = np.zeros((x.shape[0],10))\n",
        "        for i in np.arange(x.shape[0]):\n",
        "            # Forward pass through network with dropout turned off\n",
        "            output[i,:] = nn.forward(x[i,:], train_mode=False)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIfuLXkfiFG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_score(predictions, ground_truth):\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    ground_truth = np.argmax(ground_truth, axis=1)\n",
        "    return sum(predictions == ground_truth)/predictions.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SNX1RZ9X43A",
        "colab_type": "text"
      },
      "source": [
        "##Train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMtkPiT_3aUX",
        "colab_type": "text"
      },
      "source": [
        "We concluded that a network structure of two hidden layers where the first layer has 256 nodes and the second layer has 128 nodes worked well.\n",
        "\n",
        "Using softmax for the final layer with cross-entropy loss yielded approximately the same results as using relu for the final layer with MSE loss.\n",
        "\n",
        "A momentum term ($\\gamma$ in our report) equal to 0.9 also worked well.\n",
        "\n",
        "We found that weight decay and batch normalization only worsened the performance of our network, while dropout improved the performance. We settled on a probability of 0.3 to NOT retain a unit.\n",
        "\n",
        "An optimal value of the learning rate was found to be around 0.001, an adequate batch size was found to be 128 and the loss stopped decreasing well before epoch 50, so we settled on 50 epochs.\n",
        "\n",
        "This was the network structure and parameters used when producing the labels for the test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHkCvX6C7Tj9",
        "colab_type": "text"
      },
      "source": [
        "The code just below is used for evaluating the models performance on the training data. When training the model on the training data and evaluating the performance on the test data which we lack the labels for, please use the code at the bottom of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu7RlvZYTQVV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "abcee0a1-10aa-4926-a703-a97123d01ef1"
      },
      "source": [
        "# Change to true if you want to see detailed results (takes much longer).\n",
        "eval_mode = False\n",
        "\n",
        "# Create network (reduce size of layers if you think it takes too long)\n",
        "nn = MLP([128,256,128,10], [None,'relu','relu','softmax'], momentum_term=0.9,\n",
        "         weight_decay=0, dropout=0.5, batch_normalization=False)\n",
        "\n",
        "# Time the training\n",
        "start = time.time()\n",
        "\n",
        "# Train network\n",
        "if eval_mode:\n",
        "    results = nn.fit(X_train, y_train, learning_rate=1e-3,\n",
        "              batch_size=128, epochs=50, eval_mode=True, X_test=X_test, y_test=y_test)\n",
        "else:\n",
        "    results = nn.fit(X_train, y_train, learning_rate=1e-3,\n",
        "              batch_size=128, epochs=50, eval_mode=False)  \n",
        "    \n",
        "end = time.time()\n",
        "\n",
        "print('Finished training')\n",
        "print('Time to train network: %.1f seconds' %(end-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished training\n",
            "Time to train network: 345.3 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB0SiYSAu-uT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4947a65e-b051-47da-a34b-4104b8d6d4e9"
      },
      "source": [
        "# Present results\n",
        "\n",
        "# Print accuracy if test data and test labels are provided\n",
        "if (X_test is not None) and (y_test is not None):\n",
        "    test_predictions = nn.predict(X_test)\n",
        "    print(\"Accuracy on test set: %.4f\" %accuracy_score(test_predictions, y_test))\n",
        "\n",
        "# If eval mode was NOT used, plot only the loss\n",
        "if not eval_mode:\n",
        "    pl.figure(figsize=(15,4))\n",
        "    pl.plot(results)\n",
        "    pl.xlabel('Epochs')\n",
        "    pl.ylabel('Loss')\n",
        "    pl.grid()\n",
        "\n",
        "# If eval mode was used, plot training set and test set accuracy\n",
        "else:\n",
        "    pl.figure(figsize=(15,4))\n",
        "    pl.plot(results[:,1], label='Training set')\n",
        "    pl.plot(results[:,2], label='Test test')\n",
        "    pl.xlabel('Epochs')\n",
        "    pl.ylabel('Accuracy')\n",
        "    pl.legend()\n",
        "    pl.grid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRlfybKeTVSg",
        "colab_type": "text"
      },
      "source": [
        "The code below was used to produce the labels for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok4_MYjy7sb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7e673a0d-c503-4c62-c100-b25f9dd65385"
      },
      "source": [
        "test_data_id = '1ggYsoPpMztw2FM8wFEgk8HqXFumjDZ79'\n",
        "\n",
        "# Download training data\n",
        "drive = GoogleDrive(gauth)\n",
        "downloaded = drive.CreateFile({'id':test_data_id}) \n",
        "downloaded.GetContentFile('test_128.h5', 'r')\n",
        "with h5py.File('test_128.h5') as H:\n",
        "    data = np.copy(H['data'])\n",
        "\n",
        "X_test = data\n",
        "test_predictions = nn.predict(X_test)\n",
        "test_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "hf = h5py.File('/content/drive/My Drive/Predicted_labels.h5', 'w')\n",
        "hf.create_dataset('label', data=test_labels)\n",
        "hf.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7hqti1E3YJz",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}